{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6195325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train.npz: 6144000\n",
      "Archive label IDs: [64, 65, 66, 67, 68, 69, 70]\n",
      "Keeping 573394 archive‐type fragments out of 6144000 total.\n",
      "→ Copied all audio fragments.\n",
      "✅ audio only train set written to ./dataset/audio_only\\train.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import json  # <-- Add this import\n",
    "\n",
    "# 1) Paths\n",
    "SRC_NPZ   = './dataset/4k_1/train.npz'\n",
    "DST_DIR   = './dataset/audio_only'\n",
    "DST_X_NPY = os.path.join(DST_DIR, 'train_x.npy')\n",
    "DST_Y_NPY = os.path.join(DST_DIR, 'train_y.npy')\n",
    "DST_NPZ   = os.path.join(DST_DIR, 'train.npz')\n",
    "os.makedirs(DST_DIR, exist_ok=True)\n",
    "\n",
    "# 2) Load only the labels (small) to find indices\n",
    "with np.load(SRC_NPZ) as data:\n",
    "    y = data['y']                # shape = (N,), dtype uint8\n",
    "print(f\"Total samples in train.npz: {len(y)}\")\n",
    "\n",
    "# 3) Define archive class names & map to label IDs\n",
    "label_names = [ 'aiff', 'flac', 'm4a', 'mp3', 'ogg', 'wav', 'wma']\n",
    "# (make sure these exactly match your classes.json for scenario 1!)\n",
    "# Load the full list of 75 names so we can get their integer IDs:\n",
    "with open('classes.json') as f:\n",
    "    all_labels = json.load(f)['1']    # scenario “1” list of 75 names\n",
    "archive_ids = [ all_labels.index(name) for name in label_names ]\n",
    "print(\"Archive label IDs:\", archive_ids)\n",
    "\n",
    "# 4) Compute which indices to keep\n",
    "mask = np.isin(y, archive_ids)\n",
    "count = mask.sum()\n",
    "print(f\"Keeping {count} archive‐type fragments out of {len(y)} total.\")\n",
    "\n",
    "# 5) Extract the raw 'x' array to disk so we can mmap it\n",
    "#    (this only needs to happen once; next runs you can skip if memmap file exists)\n",
    "MEMMAP_X = './dataset/4k_1/train_x.npy'\n",
    "if not os.path.exists(MEMMAP_X):\n",
    "    print(\"→ Extracting x.npy from train.npz to disk…\")\n",
    "    with zipfile.ZipFile(SRC_NPZ) as zf:\n",
    "        # find the entry for 'x' (could be 'x.npy' inside the zip)\n",
    "        member = [m for m in zf.namelist() if m.startswith('x') and m.endswith('.npy')][0]\n",
    "        zf.extract(member, os.path.dirname(MEMMAP_X))\n",
    "        os.rename(os.path.join(os.path.dirname(MEMMAP_X), member), MEMMAP_X)\n",
    "\n",
    "# 6) Open x via memmap\n",
    "x = np.memmap(MEMMAP_X, mode='r', dtype=np.uint8, shape=(len(y), 4096))\n",
    "\n",
    "# 7) Allocate new memmap for just archives\n",
    "out_x = np.memmap(DST_X_NPY,\n",
    "                  mode='w+',\n",
    "                  dtype=np.uint8,\n",
    "                  shape=(count, 4096))\n",
    "out_y = np.empty((count,), dtype=y.dtype)\n",
    "\n",
    "# 8) Copy in a single pass\n",
    "idx_out = 0\n",
    "for idx_in, keep in enumerate(mask):\n",
    "    if keep:\n",
    "        out_x[idx_out] = x[idx_in]\n",
    "        out_y[idx_out] = y[idx_in]\n",
    "        idx_out += 1\n",
    "print(\"→ Copied all audio fragments.\")\n",
    "\n",
    "# 9) Flush to disk\n",
    "out_x.flush()\n",
    "np.save(DST_Y_NPY, out_y)\n",
    "\n",
    "# 10) (Optional) bundle into a single .npz for convenience\n",
    "np.savez(DST_NPZ, x=out_x, y=out_y)\n",
    "print(f\"✅ audio only train set written to {DST_NPZ}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03cf0a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive label IDs: [64, 65, 66, 67, 68, 69, 70]\n",
      "Total samples in val.npz: 768000\n",
      "Keeping 71797 audio‐type fragments out of 768000 total.\n",
      "→ Copied all audio fragments from val.\n",
      "✅ audio-only val set written to ./dataset/audio_only\\val.npz\n",
      "Total samples in test.npz: 768000\n",
      "Keeping 71609 audio‐type fragments out of 768000 total.\n",
      "→ Copied all audio fragments from test.\n",
      "✅ audio-only test set written to ./dataset/audio_only\\test.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import json  # <-- Add this import\n",
    "\n",
    "# 1) Define your archive class names & map to label IDs\n",
    "label_names = [ 'aiff', 'flac', 'm4a', 'mp3', 'ogg', 'wav', 'wma']\n",
    "# (make sure these exactly match your classes.json for scenario 1!)\n",
    "# Load the full list of 75 names so we can get their integer IDs:\n",
    "with open('classes.json') as f:\n",
    "    all_labels = json.load(f)['1']  # scenario “1” list of 75 names\n",
    "archive_ids = [all_labels.index(name) for name in label_names]\n",
    "print(\"Archive label IDs:\", archive_ids)\n",
    "\n",
    "\n",
    "# 2) Function to extract subsets for val and test\n",
    "def extract_subset(split):\n",
    "    SRC_NPZ = f'./dataset/4k_1/{split}.npz'\n",
    "    DST_DIR = './dataset/audio_only'\n",
    "    DST_X_NPY = os.path.join(DST_DIR, f'{split}_x.npy')\n",
    "    DST_Y_NPY = os.path.join(DST_DIR, f'{split}_y.npy')\n",
    "    DST_NPZ = os.path.join(DST_DIR, f'{split}.npz')\n",
    "    os.makedirs(DST_DIR, exist_ok=True)\n",
    "\n",
    "    # 3) Load only the labels (small) to find indices\n",
    "    with np.load(SRC_NPZ) as data:\n",
    "        y = data['y']  # shape = (N,), dtype uint8\n",
    "    print(f\"Total samples in {split}.npz: {len(y)}\")\n",
    "\n",
    "    # 4) Compute which indices to keep\n",
    "    mask = np.isin(y, archive_ids)\n",
    "    count = mask.sum()\n",
    "    print(f\"Keeping {count} audio‐type fragments out of {len(y)} total.\")\n",
    "\n",
    "    # 5) Extract the raw 'x' array to disk so we can mmap it (only once)\n",
    "    MEMMAP_X = f'./dataset/4k_1/{split}_x.npy'\n",
    "    if not os.path.exists(MEMMAP_X):\n",
    "        print(f\"→ Extracting {split}_x.npy from {split}.npz to disk…\")\n",
    "        with zipfile.ZipFile(SRC_NPZ) as zf:\n",
    "            # find the entry for 'x' (could be 'x.npy' inside the zip)\n",
    "            member = [m for m in zf.namelist() if m.startswith('x') and m.endswith('.npy')][0]\n",
    "            zf.extract(member, os.path.dirname(MEMMAP_X))\n",
    "            os.rename(os.path.join(os.path.dirname(MEMMAP_X), member), MEMMAP_X)\n",
    "\n",
    "    # 6) Open x via memmap\n",
    "    x = np.memmap(MEMMAP_X, mode='r', dtype=np.uint8, shape=(len(y), 4096))\n",
    "\n",
    "    # 7) Allocate new memmap for just archives\n",
    "    out_x = np.memmap(DST_X_NPY,\n",
    "                      mode='w+',\n",
    "                      dtype=np.uint8,\n",
    "                      shape=(count, 4096))\n",
    "    out_y = np.empty((count,), dtype=y.dtype)\n",
    "\n",
    "    # 8) Copy in a single pass\n",
    "    idx_out = 0\n",
    "    for idx_in, keep in enumerate(mask):\n",
    "        if keep:\n",
    "            out_x[idx_out] = x[idx_in]\n",
    "            out_y[idx_out] = y[idx_in]\n",
    "            idx_out += 1\n",
    "    print(f\"→ Copied all audio fragments from {split}.\")\n",
    "\n",
    "    # 9) Flush to disk\n",
    "    out_x.flush()\n",
    "    np.save(DST_Y_NPY, out_y)\n",
    "\n",
    "    # 10) (Optional) bundle into a single .npz for convenience\n",
    "    np.savez(DST_NPZ, x=out_x, y=out_y)\n",
    "    print(f\"✅ audio-only {split} set written to {DST_NPZ}\")\n",
    "\n",
    "\n",
    "# 11) Run for val and test\n",
    "for split in ['val', 'test']:\n",
    "    extract_subset(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238b73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 1) Define your archive labels (must match exactly the ones you filtered)\n",
    "\n",
    "label_names = [ 'aiff', 'flac', 'm4a', 'mp3', 'ogg', 'wav', 'wma']\n",
    "\n",
    "def load(split='train', data_dir='./dataset/audio_only'):\n",
    "    # 1) load raw x,y\n",
    "    npz = np.load(os.path.join(data_dir, f\"{split}.npz\"), mmap_mode='r')\n",
    "    x, y = npz['x'], npz['y']   # x.shape = (N,4096), y in [27..39]\n",
    "\n",
    "    # 2) build mapping from original IDs to 0–12\n",
    "    with open('classes.json') as f:\n",
    "        all_labels = json.load(f)['1']   # list of 75 names\n",
    "    archive_ids = [ all_labels.index(name) for name in label_names ]\n",
    "    id2new = { orig:i for i,orig in enumerate(archive_ids) }\n",
    "\n",
    "    # 3) remap y\n",
    "    #    we can do it in-place since y is small\n",
    "    y_remapped = np.vectorize(id2new.get)(y)\n",
    "    # 4) sanity check\n",
    "    assert y_remapped.min() == 0 and y_remapped.max() == len(label_names)-1\n",
    "\n",
    "    return x, y_remapped, label_names\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# x_train, y_train, archive_labels = load_archives('train')\n",
    "# x_val,   y_val,   _                = load_archives('val')\n",
    "# x_test,  y_test,  _                = load_archives('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d3b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, labels = load('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1322b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, dtype=torch.uint8) tensor(6, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.uint8)  # assuming x contains int byte values (0-255 + padding)\n",
    "del x\n",
    "y_tensor = torch.tensor(y, dtype=torch.uint8)\n",
    "del y\n",
    "train_dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "print(torch.min(y_tensor), torch.max(y_tensor))  # Print the minimum and maximum of your labels\n",
    "del x_tensor\n",
    "del y_tensor\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True,num_workers=6)\n",
    "del train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e69124c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiff', 'flac', 'm4a', 'mp3', 'ogg', 'wav', 'wma']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcfaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, labels = load('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490b779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.uint8)  # assuming x contains int byte values (0-255 + padding)\n",
    "del x\n",
    "y_tensor = torch.tensor(y, dtype=torch.uint8)\n",
    "del y\n",
    "test_dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "del x_tensor\n",
    "del y_tensor\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False,num_workers=6)\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569cd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, labels = load('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc5e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.uint8)  # assuming x contains int byte values (0-255 + padding)\n",
    "del x\n",
    "y_tensor = torch.tensor(y, dtype=torch.uint8)\n",
    "del y\n",
    "val_dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "del x_tensor\n",
    "del y_tensor\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False,num_workers=6)\n",
    "del val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674421cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        # gru_output: (B, L, H)\n",
    "        attn_weights = F.softmax(self.attn(gru_output), dim=1)  # (B, L, 1)\n",
    "        context = torch.sum(attn_weights * gru_output, dim=1)   # (B, H)\n",
    "        return context\n",
    "\n",
    "class CNN_GRU_Attn_Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_GRU_Attn_Classifier, self).__init__()\n",
    "\n",
    "        self.embedding_dim = 64\n",
    "        self.vocab_size = 257  # 0–255 + 1 for PAD\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=256\n",
    "        )\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(self.embedding_dim, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size=256, hidden_size=128, num_layers=1,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = Attention(hidden_dim=128 * 2)  # Bidirectional GRU output\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)        # (B, L, D)\n",
    "        x = x.permute(0, 2, 1)       # (B, D, L)\n",
    "\n",
    "        x = self.pool(F.gelu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.gelu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.gelu(self.bn3(self.conv3(x))))  # (B, 256, L_out)\n",
    "\n",
    "        x = x.permute(0, 2, 1)       # (B, L_out, 256)\n",
    "        gru_out, _ = self.gru(x)     # (B, L_out, 2*128)\n",
    "\n",
    "        x = self.attention(gru_out)  # (B, 2*128)\n",
    "\n",
    "        x = self.dropout(F.gelu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa527c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b12f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4fe4404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aiff', 'flac', 'm4a', 'mp3', 'ogg', 'wav', 'wma']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ead49bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ff1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_GRU_Attn_Classifier(num_classes=7).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ec9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, factor=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c09c561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "from helper_functions import accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e59929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device=device):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Wrap your DataLoader in tqdm\n",
    "        batch_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for X, y in batch_iter:\n",
    "            # send to device and cast to long only per‑batch\n",
    "            X = X.to(device).long()            \n",
    "            y = y.to(device).long()\n",
    "            \n",
    "            # forward / backward\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            current_n = batch_iter.n if batch_iter.n > 0 else 1\n",
    "            batch_iter.set_postfix(loss=total_loss / current_n)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"→ Epoch {epoch+1} complete. Avg Loss: {avg_loss:.4f}\")\n",
    "        # scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device=device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.inference_mode():\n",
    "        test_loss, test_acc = 0, 0\n",
    "        \n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(torch.long).to(device), y.to(torch.long).to(device)\n",
    "    \n",
    "\n",
    "            test_pred = model(X)  # Forward pass\n",
    "            loss = criterion(test_pred, y)  # Compute loss\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "            \n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc /= len(test_loader)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n",
    "    scheduler.step(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c15d9c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 8960/8960 [25:33<00:00,  5.84batch/s, loss=0.0528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1 complete. Avg Loss: 0.0528\n",
      "Test Loss: 0.0224, Accuracy: 99.33%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)\n",
    "test_model(model, val_loader, criterion)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/8960 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)\n",
    "test_model(model, val_loader, criterion)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28b3fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./models/FFTaudio.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/FFTexecutables.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_and_most_confused(model, dataloader, classes, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device).to(torch.long)\n",
    "            y = y.to(device).to(torch.long)\n",
    "\n",
    "            outputs = model(X)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=new_classes, yticklabels=new_classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Find most confused class pairs (excluding correct predictions)\n",
    "    cm_copy = cm.copy()\n",
    "    np.fill_diagonal(cm_copy, 0)  # Ignore diagonal (correct predictions)\n",
    "\n",
    "    confused_pairs = []\n",
    "    for true_idx in range(len(classes)):\n",
    "        for pred_idx in range(len(classes)):\n",
    "            if cm_copy[true_idx, pred_idx] > 0:\n",
    "                confused_pairs.append((cm_copy[true_idx, pred_idx], classes[true_idx], classes[pred_idx]))\n",
    "\n",
    "    confused_pairs.sort(reverse=True)\n",
    "\n",
    "    print(\"\\nTop 5 Most Confused Class Pairs:\")\n",
    "    for count, true_class, pred_class in confused_pairs[:5]:\n",
    "        print(f\"True '{true_class}' → Predicted '{pred_class}': {count} times\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['apk', 'jar', 'msi', 'dmg', '7z', 'bz2', 'deb', 'gz', 'pkg', 'rar', 'rpm', 'xz', 'zip']\n",
    "\n",
    "\n",
    "plot_confusion_and_most_confused(model, val_loader, new_classes, device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original classes\n",
    "classes = ['apk', 'jar', 'msi', 'dmg', '7z', 'bz2', 'deb', 'gz', 'pkg', 'rar', 'rpm', 'xz', 'zip']\n",
    "\n",
    "# Define merging rules\n",
    "merge_map = {\n",
    "    'xz': 'compressed',\n",
    "    '7z': 'compressed',\n",
    "    'deb': 'compressed',\n",
    "    'bz2': 'archive',\n",
    "    'dmg': 'archive'\n",
    "}\n",
    "\n",
    "# Create the final list of new classes\n",
    "new_classes = sorted(set(merge_map.values()).union(set([cls for cls in classes if cls not in merge_map])))\n",
    "print(\"New Classes:\", new_classes)\n",
    "\n",
    "# Function to map old labels to new ones\n",
    "def remap_labels(old_labels):\n",
    "    return [merge_map.get(label, label) for label in old_labels]\n",
    "\n",
    "# Example usage\n",
    "# Suppose `y_true` and `y_pred` are your true and predicted labels\n",
    "# y_true_new = remap_labels(y_true)\n",
    "# y_pred_new = remap_labels(y_pred)\n",
    "\n",
    "# Update your model's num_classes accordingly:\n",
    "# num_classes = len(new_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c1842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6105e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the remap_labels function\n",
    "def remap_labels(old_labels):\n",
    "    return [merge_map.get(label, label) for label in old_labels]\n",
    "\n",
    "# Before evaluating:\n",
    "y_true_remapped = remap_labels(y_true)\n",
    "y_pred_remapped = remap_labels(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98750fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your merge_map\n",
    "merge_map = {\n",
    "    '7z': '7z/xz/deb',\n",
    "    'xz': '7z/xz/deb',\n",
    "    'deb': '7z/xz/deb',\n",
    "    'bz2': 'bz2/dmg',\n",
    "    'dmg': 'bz2/dmg',\n",
    "    'zip': 'zip/rar',\n",
    "    'rar': 'zip/rar'\n",
    "}\n",
    "\n",
    "# 2. Define remap function\n",
    "def remap_labels(old_labels):\n",
    "    return [merge_map.get(label, label) for label in old_labels]\n",
    "\n",
    "# 3. Initialize arrays to collect true and predicted labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# 4. Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 5. No gradient tracking\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:   # or test_loader\n",
    "        X_batch = X_batch.to(device)\n",
    "        X_batch = X_batch.to(torch.long)\n",
    "        y_batch = y_batch.to(device).to(torch.long)\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# 6. Now you have arrays\n",
    "y_true = [classes[i] for i in all_labels]\n",
    "y_pred = [classes[i] for i in all_preds]\n",
    "\n",
    "# 7. Remap\n",
    "y_true_remapped = remap_labels(y_true)\n",
    "y_pred_remapped = remap_labels(y_pred)\n",
    "\n",
    "# 8. New class labels after merging\n",
    "new_classes = sorted(list(set(y_true_remapped)))\n",
    "\n",
    "# 9. Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get the raw confusion matrix\n",
    "cm = confusion_matrix(y_true_remapped, y_pred_remapped, labels=new_classes)\n",
    "\n",
    "# Step 2: Normalize it row-wise (so each row sums to 1)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Step 3: Plot it\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues',\n",
    "            xticklabels=new_classes, yticklabels=new_classes)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Normalized Confusion Matrix (After Merging)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# Merge mapping: old label name → new label int\n",
    "MERGE_GROUPS = {\n",
    "    0: ['7z', 'xz', 'deb'],\n",
    "    1: ['bz2', 'dmg'],\n",
    "    2: ['zip', 'rar'],\n",
    "}\n",
    "\n",
    "# Flatten for lookup: archive name → merged label\n",
    "merge_name2label = {}\n",
    "for new_label, names in MERGE_GROUPS.items():\n",
    "    for n in names:\n",
    "        merge_name2label[n] = new_label\n",
    "\n",
    "print(\"Merged archive groups:\", merge_name2label)\n",
    "\n",
    "def merge_and_save(split):\n",
    "    SRC = f'./dataset/archives_only/{split}.npz'\n",
    "    DST_DIR = './dataset/merged_archives'\n",
    "    os.makedirs(DST_DIR, exist_ok=True)\n",
    "    DST_X = os.path.join(DST_DIR, f'{split}_x.npy')\n",
    "    DST_Y = os.path.join(DST_DIR, f'{split}_y.npy')\n",
    "    DST_NPZ = os.path.join(DST_DIR, f'{split}.npz')\n",
    "\n",
    "    # 1. Load all labels\n",
    "    with open('classes.json') as f:\n",
    "        all_labels = json.load(f)['1']\n",
    "\n",
    "    # 2. Compute old label id → merged label mapping\n",
    "    merge_id2label = {}\n",
    "    for name, newlab in merge_name2label.items():\n",
    "        idx = all_labels.index(name)\n",
    "        merge_id2label[idx] = newlab\n",
    "\n",
    "    print(f\"{split}: Merging these label ids:\", merge_id2label)\n",
    "\n",
    "    # 3. Load y and filter indices\n",
    "    with np.load(SRC) as data:\n",
    "        y = data['y']\n",
    "        N = len(y)\n",
    "    keep_mask = np.ones_like(y, dtype=bool)\n",
    "    count = N\n",
    "    print(f\"{split}: Keeping {count} of {N} samples.\")\n",
    "\n",
    "    # 4. Memmap x if needed\n",
    "    MEMMAP_X = SRC.replace('.npz', '_x.npy')\n",
    "    if not os.path.exists(MEMMAP_X):\n",
    "        print(f\"→ Extracting {MEMMAP_X} from {SRC} ...\")\n",
    "        with zipfile.ZipFile(SRC) as zf:\n",
    "            member = [m for m in zf.namelist() if m.startswith('x') and m.endswith('.npy')][0]\n",
    "            zf.extract(member, os.path.dirname(MEMMAP_X))\n",
    "            os.rename(os.path.join(os.path.dirname(MEMMAP_X), member), MEMMAP_X)\n",
    "    x = np.memmap(MEMMAP_X, mode='r', dtype=np.uint8, shape=(N, 4096))\n",
    "\n",
    "    # 5. Allocate output\n",
    "    out_x = np.memmap(DST_X, mode='w+', dtype=np.uint8, shape=(count, 4096))\n",
    "    out_y = np.empty((count,), dtype=np.uint8)\n",
    "\n",
    "    # 6. Copy and relabel\n",
    "    # 6. Copy and relabel\n",
    "    idx_out = 0\n",
    "    for idx_in in range(N):\n",
    "        if keep_mask[idx_in]:\n",
    "            out_x[idx_out] = x[idx_in]\n",
    "            orig_label = y[idx_in]\n",
    "            # If it's in our merge map, merge it. Otherwise, keep as is.\n",
    "            out_y[idx_out] = merge_id2label.get(orig_label, orig_label)\n",
    "            idx_out += 1\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    merge_and_save(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b0bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
