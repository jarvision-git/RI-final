{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e5377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Office labels: ['doc', 'docx', 'key', 'ppt', 'pptx', 'xls', 'xlsx']\n",
      "Processing NPZ file: ./dataset\\4k_1\\train.npz\n",
      "Loaded 'y' array, shape: (6144000,)\n",
      "Found existing memmap file at: ./dataset\\4k_1\\train_x.npy\n",
      "Opening memmap for 'x': ./dataset\\4k_1\\train_x.npy with shape (6144000, 4096)\n",
      "Memmap for 'x' opened successfully.\n",
      "Found 'doc' at Scenario 1 index: 44 -> Specialist index: 0\n",
      "Found 'docx' at Scenario 1 index: 45 -> Specialist index: 1\n",
      "Found 'key' at Scenario 1 index: 46 -> Specialist index: 2\n",
      "Found 'ppt' at Scenario 1 index: 47 -> Specialist index: 3\n",
      "Found 'pptx' at Scenario 1 index: 48 -> Specialist index: 4\n",
      "Found 'xls' at Scenario 1 index: 49 -> Specialist index: 5\n",
      "Found 'xlsx' at Scenario 1 index: 50 -> Specialist index: 6\n",
      "\n",
      "Scenario 1 indices for Office types: [44, 45, 46, 47, 48, 49, 50]\n",
      "Mapping from S1 index to Specialist index: {44: 0, 45: 1, 46: 2, 47: 3, 48: 4, 49: 5, 50: 6}\n",
      "Final Specialist Labels: ['doc', 'docx', 'key', 'ppt', 'pptx', 'xls', 'xlsx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch # Keep torch for later tensor conversion if needed\n",
    "\n",
    "# --- Ensure the memory-mapping load function is defined ---\n",
    "import zipfile\n",
    "def load(scenario=1, block_size='4k', subset='train'):\n",
    "    # (Use the robust load function definition from previous answers\n",
    "    #  that uses zipfile.extract + np.memmap)\n",
    "    # Validate parameters\n",
    "    if block_size not in ['512', '4k']:\n",
    "        raise ValueError('Invalid block size!')\n",
    "    if scenario not in range(1, 7):\n",
    "        raise ValueError('Invalid scenario!')\n",
    "    if subset not in ['train', 'val', 'test']:\n",
    "        raise ValueError('Invalid subset!')\n",
    "\n",
    "    # Build data directory and NPZ path\n",
    "    data_dir = os.path.join('./dataset', '{:s}_{:1d}'.format(block_size, scenario))\n",
    "    npz_path = os.path.join(data_dir, '{}.npz'.format(subset))\n",
    "    print(f\"Processing NPZ file: {npz_path}\")\n",
    "\n",
    "    # Load classes information from classes.json\n",
    "    if os.path.isfile('classes.json'):\n",
    "        with open('classes.json') as json_file:\n",
    "            classes = json.load(json_file)\n",
    "            labels_list = classes[str(scenario)] # Get list of labels for the scenario\n",
    "    else:\n",
    "        raise FileNotFoundError('Please download classes.json to the current directory!')\n",
    "\n",
    "    # Load y from the NPZ (this is small)\n",
    "    try:\n",
    "        with np.load(npz_path, allow_pickle=True) as data_zip:\n",
    "            y = data_zip['y']\n",
    "        print(f\"Loaded 'y' array, shape: {y.shape}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load 'y' from {npz_path}: {e}\")\n",
    "\n",
    "    num_samples = y.shape[0]\n",
    "\n",
    "    # Determine sample length based on block_size\n",
    "    sample_len = 4096 if block_size == '4k' else 512\n",
    "    desired_shape = (num_samples, sample_len)\n",
    "\n",
    "    # Define path for the npy file that will be used for memmapping x\n",
    "    memmap_path = os.path.join(data_dir, '{}_x.npy'.format(subset))\n",
    "\n",
    "    # If memmap file does not exist, extract it from the NPZ\n",
    "    if not os.path.exists(memmap_path):\n",
    "        print(f\"Extracting 'x' data to memmap file: {memmap_path}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(npz_path, 'r') as z:\n",
    "                name = None\n",
    "                for filename in z.namelist():\n",
    "                    if filename.startswith('x') and filename.endswith('.npy'):\n",
    "                        name = filename\n",
    "                        break\n",
    "                if name is None:\n",
    "                    # Fallback if 'x.npy' isn't found directly (might be just 'x'?)\n",
    "                    if 'x' in z.namelist(): name = 'x' # Check if key is just 'x'\n",
    "                    else: raise KeyError(\"No file/key for 'x' found in the NPZ archive.\")\n",
    "\n",
    "                print(f\"Found '{name}' in archive, extracting...\")\n",
    "                # Define temporary path for extraction\n",
    "                temp_extract_path = os.path.join(data_dir, name)\n",
    "\n",
    "                # Ensure the target directory exists\n",
    "                os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "                # Extract directly\n",
    "                with open(temp_extract_path, 'wb') as f_out, z.open(name) as f_in:\n",
    "                     f_out.write(f_in.read())\n",
    "                print(f\"Extracted to temporary path: {temp_extract_path}\")\n",
    "\n",
    "                # Rename the extracted file to the desired memmap path\n",
    "                os.rename(temp_extract_path, memmap_path)\n",
    "                print(f\"Renamed to memmap path: {memmap_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Clean up potentially partially extracted file on error\n",
    "            if os.path.exists(temp_extract_path): os.remove(temp_extract_path)\n",
    "            if os.path.exists(memmap_path): os.remove(memmap_path)\n",
    "            raise RuntimeError(f\"Failed to extract 'x' from {npz_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Found existing memmap file at: {memmap_path}\")\n",
    "\n",
    "    # Open a memmap for x\n",
    "    try:\n",
    "        print(f\"Opening memmap for 'x': {memmap_path} with shape {desired_shape}\")\n",
    "        x = np.memmap(memmap_path, mode='r', dtype=np.uint8, shape=desired_shape)\n",
    "        print(\"Memmap for 'x' opened successfully.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create memmap for {memmap_path} with shape {desired_shape}: {e}\")\n",
    "\n",
    "    return x, y, labels_list # Return the list of labels too\n",
    "\n",
    "# --- Define Target Office Labels ---\n",
    "office_labels_specialist = ['doc', 'docx', 'key', 'ppt', 'pptx', 'xls', 'xlsx']\n",
    "print(f\"Target Office labels: {office_labels_specialist}\")\n",
    "\n",
    "# --- Load Scenario 1 Labels to find indices ---\n",
    "try:\n",
    "    _, _, s1_labels_list = load(scenario=1, block_size='4k', subset='train') # Load just to get labels\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Cannot find Scenario 1 data or classes.json: {e}\")\n",
    "    exit() # Stop if we can't get S1 labels\n",
    "\n",
    "# --- Find S1 indices for Office types ---\n",
    "s1_office_indices = []\n",
    "specialist_label_map = {} # S1 index -> Specialist index (0-6)\n",
    "new_specialist_labels = [] # Store the labels in the new order\n",
    "\n",
    "for new_idx, label in enumerate(office_labels_specialist):\n",
    "    try:\n",
    "        s1_idx = s1_labels_list.index(label)\n",
    "        s1_office_indices.append(s1_idx)\n",
    "        specialist_label_map[s1_idx] = new_idx\n",
    "        new_specialist_labels.append(label)\n",
    "        print(f\"Found '{label}' at Scenario 1 index: {s1_idx} -> Specialist index: {new_idx}\")\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Label '{label}' not found in Scenario 1 labels!\")\n",
    "\n",
    "print(f\"\\nScenario 1 indices for Office types: {s1_office_indices}\")\n",
    "print(f\"Mapping from S1 index to Specialist index: {specialist_label_map}\")\n",
    "print(f\"Final Specialist Labels: {new_specialist_labels}\") # Should match office_labels_specialist\n",
    "\n",
    "if len(s1_office_indices) != len(office_labels_specialist):\n",
    "    print(\"Error: Not all Office labels were found in Scenario 1. Cannot proceed.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e332ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_specialist_dataset(source_scenario, source_block_size, subset,\n",
    "                              target_indices_s1, target_label_map_s1,\n",
    "                              output_dir, output_prefix=\"office_specialist\"):\n",
    "    \"\"\"\n",
    "    Creates a specialist dataset by filtering Scenario 1 data.\n",
    "\n",
    "    Args:\n",
    "        source_scenario (int): Scenario number of the source data (should be 1).\n",
    "        source_block_size (str): Block size ('4k' or '512').\n",
    "        subset (str): Data split ('train', 'val', or 'test').\n",
    "        target_indices_s1 (list): List of indices in the source scenario's labels\n",
    "                                   that correspond to the specialist classes.\n",
    "        target_label_map_s1 (dict): Mapping from source index to new specialist index (0 to N-1).\n",
    "        output_dir (str): Directory to save the new dataset files.\n",
    "        output_prefix (str): Prefix for the output filenames.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Creating Specialist Dataset for Subset: {subset} ---\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load Scenario 1 data (x is memmapped, y is in memory)\n",
    "    try:\n",
    "        x_s1_mm, y_s1, _ = load(source_scenario, source_block_size, subset)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading source data for {subset}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Create a boolean mask to find rows matching target indices\n",
    "    print(\"Creating filter mask...\")\n",
    "    mask = np.isin(y_s1, target_indices_s1)\n",
    "    num_target_samples = np.sum(mask)\n",
    "    print(f\"Found {num_target_samples} samples for the specialist dataset in '{subset}'.\")\n",
    "\n",
    "    if num_target_samples == 0:\n",
    "        print(\"Warning: No samples found for this subset. Skipping save.\")\n",
    "        return\n",
    "\n",
    "    # Filter the 'y' array and remap labels\n",
    "    print(\"Filtering and remapping labels ('y')...\")\n",
    "    y_specialist = y_s1[mask]\n",
    "    # Apply the mapping: for each old label index, get the new one\n",
    "    y_specialist_remapped = np.array([target_label_map_s1[old_idx] for old_idx in y_specialist], dtype=np.uint8)\n",
    "    print(f\"Shape of new 'y': {y_specialist_remapped.shape}\")\n",
    "\n",
    "    # Check memory requirements before trying to filter 'x'\n",
    "    bytes_needed = num_target_samples * x_s1_mm.shape[1] * x_s1_mm.dtype.itemsize\n",
    "    gb_needed = bytes_needed / (1024**3)\n",
    "    print(f\"Estimated memory needed to load filtered 'x' into RAM: {gb_needed:.2f} GiB\")\n",
    "\n",
    "    # Define output paths\n",
    "    output_x_path = os.path.join(output_dir, f\"{output_prefix}_{subset}_x.npy\")\n",
    "    output_y_path = os.path.join(output_dir, f\"{output_prefix}_{subset}_y.npy\")\n",
    "\n",
    "    # Filter 'x' - Try direct indexing first if memory seems okay, otherwise iterate\n",
    "    # Threshold for direct indexing (e.g., 4 GiB - adjust based on your RAM)\n",
    "    direct_index_threshold_gb = 4.0\n",
    "\n",
    "    try:\n",
    "        if gb_needed < direct_index_threshold_gb:\n",
    "            print(\"Attempting direct indexing to filter 'x' into memory...\")\n",
    "            # Use .copy() to force loading into RAM and create a standard numpy array\n",
    "            x_specialist = x_s1_mm[mask].copy()\n",
    "            print(\"Direct indexing successful.\")\n",
    "\n",
    "            # Save the new arrays\n",
    "            print(f\"Saving filtered 'x' to: {output_x_path}\")\n",
    "            np.save(output_x_path, x_specialist)\n",
    "            print(f\"Saving remapped 'y' to: {output_y_path}\")\n",
    "            np.save(output_y_path, y_specialist_remapped)\n",
    "\n",
    "        else:\n",
    "            print(\"Estimated memory too large for direct indexing. Using iterative copy...\")\n",
    "            # Create a writeable memmap file for the output\n",
    "            x_specialist_shape = (num_target_samples, x_s1_mm.shape[1])\n",
    "            x_specialist_mm_out = np.memmap(output_x_path, dtype=x_s1_mm.dtype, mode='w+', shape=x_specialist_shape)\n",
    "\n",
    "            # Find the indices we need to copy\n",
    "            target_row_indices = np.where(mask)[0]\n",
    "\n",
    "            # Iterate and copy in chunks\n",
    "            chunk_size = 10000 # Adjust chunk size based on available RAM\n",
    "            start_idx_out = 0\n",
    "            for i in tqdm(range(0, num_target_samples, chunk_size), desc=\"Copying chunks\"):\n",
    "                end_idx_in_indices = min(i + chunk_size, num_target_samples)\n",
    "                indices_to_copy = target_row_indices[i:end_idx_in_indices]\n",
    "\n",
    "                # Read slice from source memmap\n",
    "                data_slice = x_s1_mm[indices_to_copy] # Read slice\n",
    "\n",
    "                # Determine corresponding output slice indices\n",
    "                num_in_slice = len(indices_to_copy)\n",
    "                end_idx_out = start_idx_out + num_in_slice\n",
    "\n",
    "                # Write slice to output memmap\n",
    "                x_specialist_mm_out[start_idx_out:end_idx_out] = data_slice\n",
    "\n",
    "                start_idx_out = end_idx_out\n",
    "\n",
    "            # Ensure data is written to disk\n",
    "            del x_specialist_mm_out # Deleting flushes and closes the memmap\n",
    "            print(\"Iterative copy complete.\")\n",
    "\n",
    "            # Save the corresponding 'y' array\n",
    "            print(f\"Saving remapped 'y' to: {output_y_path}\")\n",
    "            np.save(output_y_path, y_specialist_remapped)\n",
    "\n",
    "        print(f\"Successfully created specialist dataset files for '{subset}' in '{output_dir}'.\")\n",
    "\n",
    "    except MemoryError as e:\n",
    "        print(f\"\\nMemoryError occurred during filtering/saving 'x': {e}\")\n",
    "        print(\"Even the chosen filtering method failed. Try reducing chunk_size if using iterative copy, or ensure more system RAM/PageFile.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during filtering/saving: {e}\")\n",
    "\n",
    "    # It's good practice to explicitly delete large objects when done\n",
    "    del x_s1_mm, y_s1, mask\n",
    "    if 'x_specialist' in locals(): del x_specialist\n",
    "    if 'y_specialist' in locals(): del y_specialist\n",
    "    if 'y_specialist_remapped' in locals(): del y_specialist_remapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa53309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating Specialist Dataset for Subset: train ---\n",
      "Processing NPZ file: ./dataset\\4k_1\\train.npz\n",
      "Loaded 'y' array, shape: (6144000,)\n",
      "Found existing memmap file at: ./dataset\\4k_1\\train_x.npy\n",
      "Opening memmap for 'x': ./dataset\\4k_1\\train_x.npy with shape (6144000, 4096)\n",
      "Memmap for 'x' opened successfully.\n",
      "Creating filter mask...\n",
      "Found 573528 samples for the specialist dataset in 'train'.\n",
      "Filtering and remapping labels ('y')...\n",
      "Shape of new 'y': (573528,)\n",
      "Estimated memory needed to load filtered 'x' into RAM: 2.19 GiB\n",
      "Attempting direct indexing to filter 'x' into memory...\n",
      "Direct indexing successful.\n",
      "Saving filtered 'x' to: ./dataset/office_specialist_4k\\office_specialist_train_x.npy\n",
      "Saving remapped 'y' to: ./dataset/office_specialist_4k\\office_specialist_train_y.npy\n",
      "Successfully created specialist dataset files for 'train' in './dataset/office_specialist_4k'.\n",
      "\n",
      "--- Creating Specialist Dataset for Subset: val ---\n",
      "Processing NPZ file: ./dataset\\4k_1\\val.npz\n",
      "Loaded 'y' array, shape: (768000,)\n",
      "Found existing memmap file at: ./dataset\\4k_1\\val_x.npy\n",
      "Opening memmap for 'x': ./dataset\\4k_1\\val_x.npy with shape (768000, 4096)\n",
      "Memmap for 'x' opened successfully.\n",
      "Creating filter mask...\n",
      "Found 71554 samples for the specialist dataset in 'val'.\n",
      "Filtering and remapping labels ('y')...\n",
      "Shape of new 'y': (71554,)\n",
      "Estimated memory needed to load filtered 'x' into RAM: 0.27 GiB\n",
      "Attempting direct indexing to filter 'x' into memory...\n",
      "Direct indexing successful.\n",
      "Saving filtered 'x' to: ./dataset/office_specialist_4k\\office_specialist_val_x.npy\n",
      "Saving remapped 'y' to: ./dataset/office_specialist_4k\\office_specialist_val_y.npy\n",
      "Successfully created specialist dataset files for 'val' in './dataset/office_specialist_4k'.\n",
      "\n",
      "--- Creating Specialist Dataset for Subset: test ---\n",
      "Processing NPZ file: ./dataset\\4k_1\\test.npz\n",
      "Loaded 'y' array, shape: (768000,)\n",
      "Found existing memmap file at: ./dataset\\4k_1\\test_x.npy\n",
      "Opening memmap for 'x': ./dataset\\4k_1\\test_x.npy with shape (768000, 4096)\n",
      "Memmap for 'x' opened successfully.\n",
      "Creating filter mask...\n",
      "Found 71718 samples for the specialist dataset in 'test'.\n",
      "Filtering and remapping labels ('y')...\n",
      "Shape of new 'y': (71718,)\n",
      "Estimated memory needed to load filtered 'x' into RAM: 0.27 GiB\n",
      "Attempting direct indexing to filter 'x' into memory...\n",
      "Direct indexing successful.\n",
      "Saving filtered 'x' to: ./dataset/office_specialist_4k\\office_specialist_test_x.npy\n",
      "Saving remapped 'y' to: ./dataset/office_specialist_4k\\office_specialist_test_y.npy\n",
      "Successfully created specialist dataset files for 'test' in './dataset/office_specialist_4k'.\n",
      "\n",
      "Saved specialist labels to ./dataset/office_specialist_4k\\labels.json\n"
     ]
    }
   ],
   "source": [
    "# Define output directory for the specialist dataset\n",
    "output_specialist_dir = './dataset/office_specialist_4k'\n",
    "\n",
    "# Create the datasets\n",
    "create_specialist_dataset(\n",
    "    source_scenario=1,\n",
    "    source_block_size='4k',\n",
    "    subset='train',\n",
    "    target_indices_s1=s1_office_indices,\n",
    "    target_label_map_s1=specialist_label_map,\n",
    "    output_dir=output_specialist_dir\n",
    ")\n",
    "\n",
    "create_specialist_dataset(\n",
    "    source_scenario=1,\n",
    "    source_block_size='4k',\n",
    "    subset='val',\n",
    "    target_indices_s1=s1_office_indices,\n",
    "    target_label_map_s1=specialist_label_map,\n",
    "    output_dir=output_specialist_dir\n",
    ")\n",
    "\n",
    "create_specialist_dataset(\n",
    "    source_scenario=1,\n",
    "    source_block_size='4k',\n",
    "    subset='test',\n",
    "    target_indices_s1=s1_office_indices,\n",
    "    target_label_map_s1=specialist_label_map,\n",
    "    output_dir=output_specialist_dir\n",
    ")\n",
    "\n",
    "# Also save the new labels list\n",
    "output_labels_path = os.path.join(output_specialist_dir, 'labels.json')\n",
    "with open(output_labels_path, 'w') as f:\n",
    "    json.dump(new_specialist_labels, f)\n",
    "print(f\"\\nSaved specialist labels to {output_labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394ecdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading specialist data: ./dataset/office_specialist_4k\\office_specialist_train_x.npy, ./dataset/office_specialist_4k\\office_specialist_train_y.npy\n",
      "Loaded specialist train data: x_shape=(573528, 4096), y_shape=(573528,)\n"
     ]
    }
   ],
   "source": [
    "def load_specialist(dataset_dir, subset='train'):\n",
    "    \"\"\"Loads data from the specialist dataset directory.\"\"\"\n",
    "    x_path = os.path.join(dataset_dir, f\"office_specialist_{subset}_x.npy\")\n",
    "    y_path = os.path.join(dataset_dir, f\"office_specialist_{subset}_y.npy\")\n",
    "    labels_path = os.path.join(dataset_dir, 'labels.json')\n",
    "\n",
    "    if not (os.path.exists(x_path) and os.path.exists(y_path)):\n",
    "        raise FileNotFoundError(f\"Dataset files not found in {dataset_dir} for subset {subset}\")\n",
    "\n",
    "    print(f\"Loading specialist data: {x_path}, {y_path}\")\n",
    "    # Use np.load with mmap_mode to correctly read the header and shape\n",
    "    x = np.load(x_path, mmap_mode='r')\n",
    "    y = np.load(y_path)\n",
    "    # Load labels\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = json.load(f)\n",
    "\n",
    "    print(f\"Loaded specialist {subset} data: x_shape={x.shape}, y_shape={y.shape}\")\n",
    "    return x, y, labels\n",
    "\n",
    "# Example usage:\n",
    "x, y, labels = load_specialist(output_specialist_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ffd3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc', 'docx', 'key', 'ppt', 'pptx', 'xls', 'xlsx']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae60aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Datasets & DataLoaders\n",
    "x_tensor = torch.tensor(x, dtype=torch.uint8)  # assuming x contains int byte values (0-255 + padding)\n",
    "del x\n",
    "y_tensor = torch.tensor(y, dtype=torch.uint8)\n",
    "del y\n",
    "train_dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "del x_tensor\n",
    "del y_tensor\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True,num_workers=4,prefetch_factor=2)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=6,\n",
    ")\n",
    "\n",
    "del train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d5f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading specialist data: ./dataset/office_specialist_4k\\office_specialist_test_x.npy, ./dataset/office_specialist_4k\\office_specialist_test_y.npy\n",
      "Loaded specialist test data: x_shape=(71718, 4096), y_shape=(71718,)\n"
     ]
    }
   ],
   "source": [
    "x, y, labels = load_specialist(output_specialist_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c010b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Datasets & DataLoaders\n",
    "x_tensor = torch.tensor(x, dtype=torch.uint8)  # assuming x contains int byte values (0-255 + padding)\n",
    "del x\n",
    "y_tensor = torch.tensor(y, dtype=torch.uint8)\n",
    "del y\n",
    "test_dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "del x_tensor\n",
    "del y_tensor\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True,num_workers=4,prefetch_factor=2)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=6,\n",
    ")\n",
    "\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34599b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading specialist data: ./dataset/office_specialist_4k\\office_specialist_val_x.npy, ./dataset/office_specialist_4k\\office_specialist_val_y.npy\n",
      "Loaded specialist val data: x_shape=(71554, 4096), y_shape=(71554,)\n"
     ]
    }
   ],
   "source": [
    "x, y, labels = load_specialist(output_specialist_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d219fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Datasets & DataLoaders\n",
    "x_tensor = torch.tensor(x, dtype=torch.uint8)  # assuming x contains int byte values (0-255 + padding)\n",
    "del x\n",
    "y_tensor = torch.tensor(y, dtype=torch.uint8)\n",
    "del y\n",
    "val_dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "del x_tensor\n",
    "del y_tensor\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True,num_workers=4,prefetch_factor=2)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=6,\n",
    ")\n",
    "\n",
    "del val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ce704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        # gru_output: (B, L, H)\n",
    "        attn_weights = F.softmax(self.attn(gru_output), dim=1)  # (B, L, 1)\n",
    "        context = torch.sum(attn_weights * gru_output, dim=1)   # (B, H)\n",
    "        return context\n",
    "\n",
    "class CNN_GRU_Attn_Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_GRU_Attn_Classifier, self).__init__()\n",
    "\n",
    "        self.embedding_dim = 64\n",
    "        self.vocab_size = 257  # 0–255 + 1 for PAD\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=256\n",
    "        )\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(self.embedding_dim, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size=256, hidden_size=128, num_layers=1,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = Attention(hidden_dim=128 * 2)  # Bidirectional GRU output\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)        # (B, L, D)\n",
    "        x = x.permute(0, 2, 1)       # (B, D, L)\n",
    "\n",
    "        x = self.pool(F.gelu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.gelu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.gelu(self.bn3(self.conv3(x))))  # (B, 256, L_out)\n",
    "\n",
    "        x = x.permute(0, 2, 1)       # (B, L_out, 256)\n",
    "        gru_out, _ = self.gru(x)     # (B, L_out, 2*128)\n",
    "\n",
    "        x = self.attention(gru_out)  # (B, 2*128)\n",
    "\n",
    "        x = self.dropout(F.gelu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5881fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248ad075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d5700eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_GRU_Attn_Classifier(num_classes=7).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e60b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, factor=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d5329ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "from helper_functions import accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11b6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device=device):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Wrap your DataLoader in tqdm\n",
    "        batch_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for X, y in batch_iter:\n",
    "            # send to device and cast to long only per‑batch\n",
    "            X = X.to(device).long()\n",
    "            y = y.to(device).long()\n",
    "            \n",
    "            # forward / backward\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            current_n = batch_iter.n if batch_iter.n > 0 else 1\n",
    "            batch_iter.set_postfix(loss=total_loss / current_n)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"→ Epoch {epoch+1} complete. Avg Loss: {avg_loss:.4f}\")\n",
    "        # scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4b64a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device=device):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Wrap your DataLoader in tqdm\n",
    "        batch_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for X, y in batch_iter:\n",
    "            # send to device and cast to long only per‑batch\n",
    "            X = X.to(device).long()\n",
    "            y = y.to(device).long()\n",
    "            \n",
    "            # forward / backward\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            current_n = batch_iter.n if batch_iter.n > 0 else 1\n",
    "            batch_iter.set_postfix(loss=total_loss / current_n)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"→ Epoch {epoch+1} complete. Avg Loss: {avg_loss:.4f}\")\n",
    "        # scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cfdf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion,scheduler ,device=device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.inference_mode():\n",
    "        test_loss, test_acc = 0, 0\n",
    "        \n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(torch.long).to(device), y.to(torch.long).to(device)\n",
    "\n",
    "            test_pred = model(X)  # Forward pass\n",
    "            loss = criterion(test_pred, y)  # Compute loss\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "            \n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc /= len(test_loader)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n",
    "    scheduler.step(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fb11bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 35846/35846 [30:50<00:00, 19.37batch/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1 complete. Avg Loss: 0.2665\n",
      "Test Loss: 0.3463, Accuracy: 86.98%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)\n",
    "test_model(model, val_loader,criterion,scheduler)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74601b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./models/FFToffice-10epocs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b88776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)\n",
    "test_model(model, val_loader,criterion,scheduler)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./models/FFToffice-10epocs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e581c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/FFToffice-9epocs.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a399b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
